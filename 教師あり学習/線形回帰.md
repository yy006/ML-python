# リッジ回帰
- alpha 大 → 係数への制約 大　 適合不足に近づく
- alpha 小 → 係数への制約 小　 過剰適合に近づく(線形回帰)
図2-12

  alphaを固定して訓練データの量を変える

正則化(過剰適合を防ぐために、各特徴量が出力に与える影響を少なくする。各特徴量の係数を小さくする)は訓練データ数が小さいときに有効

線形回帰：データ数 大 → 訓練性能低下 (過剰適合することが難しい。データを覚えておくことが難しい。) 

# Lasso
Ridgeに代わる線形回帰  
L1正則化を行うため、いくつかの係数が完全に０となる。これは、いくつかの特徴量を完全に無視するということ。
    
    デフォルトの正則化パラメータでは、特徴量を無視しすぎていた。適合不足の度合いを減らすためにalphaを減らす
    
実際に使ってみる場合：リッジ回帰を試してみる。重要な特徴量が少なかった場合Lassoの方が理解しやすいモデルが得られるだろう。  
scikit-learnには、LassoとRidgeのペナルティを組み合わせたElasticNetがあり、実用上は最適な結果をもたらす。

# クラス分類のための線形モデル
## 概要
線形モデルはクラス分類にも多用されている。２クラス分類は次の式で予測を行う。  
`y = w[0]*x[0] + w[1]*x[1] + ・・・ + w[p]*x[p] + b > 0`  
- 特徴量の重み付き和をそのまま返すのではない。予測された値(y)が0より小さければ、クラスは-1になり、0より大きければ+1になる。  
- ここでも、係数`w`と切片`b`を求めるために、様々な方法がある。  

- 線形の2クラス分類器は、2つのクラスを直線や平面や超平面で分割する
- 線形モデルを学習するアルゴリズムは、以下2点で区別される
  - 係数と切片の特定の組合せと訓練データの適合度を測る尺度(？)
  - 正規化を行うか。行うならどの方法を使うか


    
 [参考](https://qiita.com/renesisu727/items/3fbed61e3253934eb68e "サポートベクトルマシン(SVM)について、できるだけ分かりやすくまとめていく④～ソフトマージンとハードマージンの実装～")
 
## 決定境界の可視化
- ロジスティック回帰(回帰アルゴリズムではなくクラス分類アルゴリズムである。)と線形サポートベクタマシンを適用する。

        メモ：
      1. モデルをfit()で学習
      2. モデルの決定境界をプロット  
      3. データ点をプロット、X[:, 0]とX[:, 1] はそれぞれ X の 第1特徴と 第2特徴を選択　　
- 正則化の強度を決定するトレードオフパラメータCは、大きくなると正則化は弱くなる。  
つまり、Cを大きくすると、それぞれのモデルは訓練データに対しての適合度を上げようとするが、Cを小さくすると係数ベクトル`w`を0に近づけることを重視するようになる。

- 小さいCを用いると、データポイントの「大多数」に対して適合しようとするが、大きいCを用いると、個々のデータポイントを正確にクラス分類することを重視するようになる。

回帰と同様に、低次元だとモデルの制約が強すぎるように感じる(モデルが単純すぎて、データを上手く表現できていない？)かもしれないが、高次元の場合には線形モデルによるクラス分類は非常に強力になる。

    (線形回帰)特徴量が多いとき訓練性能が低下する。データ量が多くなると、モデルが過剰適合する。(図2-13)

回帰のときと同じように(__L1、L2正則化のこと？__)、モデル間の主な違いはpenaltyパラメータにある。このパラメータがモデルの正則化と、特徴量をすべて使うか一部しか使わないかに影響を与える。

__L1, L2正則化とは__

-----

# 線形モデルによる多クラス分類
## 概要
多くの線形クラス分類モデルは、２クラス分類にしか適用できない。  
1対その他(one-vs.-rest)アプローチにより、2クラス分類アルゴリズムを多クラス分類アルゴリズムに拡張する。(__一般的な手法__)

`w[0]*x[0] + w[1]*x[1] + ・・・ + w[p]*x[p] + b `
この値(確信度)が最も大きい値となるクラスを、クラスラベルとして割り当てる。

    決定境界、実際のデータ、確信度の式の値が最も大きいものを～、コードの意味、のそれぞれの関係性がよくわからない

## 利点、欠点、パラメータ
線形モデルについて  

1. 主要なパラメータ：alpha(回帰モデル),C(線形SVC,ロジスティック回帰)のような正規化パラメータ
2. L1,L2正規化を使うか決める
3. 線形モデルの訓練は非常に高速で、さらに予測も高速である。
4. 非常に大きなデータセットに対して有効なオプションがある
5. 予測手法が比較的理解しやすい
6. 線形モデルは、特徴量の数がサンプルの個数よりも多いときに性能を発揮する。