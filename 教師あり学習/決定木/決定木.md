# 決定木
## 概要

-----
クラス分類と回帰タスクに幅広く用いられているモデルである。
Yes/Noで答えられる質問で構成された木構造を学習する。

## 決定木の構築

----
決定木における学習は、正解に最も早くたどり着けるような一連のYes/No型の質問の学習を意味する。  
これらの質問を __テスト__ と呼ぶ。  
- 構築する際、目的変数に対して最も情報量の多いものを選んで分割する。
- 各領域が1つのクラスしか含まなくなるまで繰り返す。

__情報量の多い分割__：分割後の子ノードの純度ができるだけ高くなるような特徴量(利得)を用いて考える

## 決定木の複雑さの制御

-----
葉が純粋になるまで分割を続けると、モデルは複雑になりすぎてしまう(過剰適合)。
- 事前枝刈り：構築過程で木の生成を早めに止める  
木の深さを制限する、葉の最大値を制限する、分割内の点の最小数を決めておく
- 事後枝刈り：一度構築してから、情報の少ないノードを削除する

scikit-learnでは回帰と分類が2つのクラスに実装されている。

決定木を検証するよい方法の1つは、大多数のデータがたどるパスを見つけること。

## 決定木の特徴量の重要性

----
木全体を見るのは大変なので、決定木の挙動を要約する特性値を見てみる。  
要約に最もよく使われるのは __特徴量の重要度__ と呼ばれ、個々の特徴量がどの程度重要かを示す割合である。

    特徴量の重要度：それぞれの特徴量に対する0と1の間の数で、0は「まったく使われていない」、1は「完全にターゲットを予想できる」を意味する。　　
    特徴量の重要度の和は常に1になる。

    ある特徴量で分割することでどれくらいジニ不純度を下げられるのか

[決定木アルゴリズムの重要度(importance)を正しく解釈しよう](https://yolo-kiyoshi.com/2019/09/16/post-1226/)

特徴量が重要であることは、どのクラスに属するかまでは教えてくれない。

## 長所、短所、パラメータ

----
決定木におけるモデルの複雑さを制御するパラメーターは、事前枝刈りパラメータ　　

長所：  
結果のモデルが容易に可視化可能、データのスケールに対して完全に不変であること　　

後者について、個々の特徴量は独立に処理され、データの分割はスケールに依存しないので、決定木においては特徴量の正規化や標準化は必要ない。　　

短所：過剰適合しやすく、汎化性能が低い傾向がある